{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ComparingNotebook.ipynb","provenance":[],"collapsed_sections":["QaOO2USY_Qqa"],"mount_file_id":"1ORDmOIpqFLbBwjJI0mMADRu3Xjfa_tHB","authorship_tag":"ABX9TyOfQmWgN9lYDu4Qj0j8DqaD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"k_IpRE6I5h4f"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"EJldyJzgCFtg"},"source":["# **Evaluation Notebook**\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TosF3Z2S_QqY"},"source":["#**Preparations**\n","\n","\n","> In this group of cells we will prepare all the variables, code & configurations that we might need while going through all the phases of increamental learning.\n","\n","** Do not run any code after this without running this group of cells **\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-aJtOBN1_QqY"},"source":["## Mount Drive\n","\n","\n",">First we need to mount the drive so we can retrieve the code and data from our storage in to the notebook.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vifTjPM3_QqZ","executionInfo":{"status":"ok","timestamp":1643826511937,"user_tz":-300,"elapsed":28900,"user":{"displayName":"usama shahid","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03571715957292202472"}},"outputId":"398d9b80-90c9-4d7e-8fca-b071ad9080c8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"QaOO2USY_Qqa"},"source":["## Install Necessary Packages\n","\n","\n","> We need to install the packages that are needed for this model. Most of the packages are already installed in google colab. We will install the packages which are either not installed or we need some specific version of the package.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Ix84P-S_Qqa","executionInfo":{"status":"ok","timestamp":1643826531713,"user_tz":-300,"elapsed":19793,"user":{"displayName":"usama shahid","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03571715957292202472"}},"outputId":"33fe4972-2bc3-49ab-c2c0-e21613f14647"},"source":["# keras version 2.0.8,\n","# tensorflow version 1.5 ,\n","# version mrcnn 0.1 works!\n","!pip install h5py==2.10.0\n","for _ in range(2):\n"," !pip install keras==2.1.0\n","!pip install -U scikit-image==0.16.2\n","%tensorflow_version 1.x \n","# !pip install mrcnn==0.1\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting h5py==2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n","Installing collected packages: h5py\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","Successfully installed h5py-2.10.0\n","Collecting keras==2.1.0\n","  Downloading Keras-2.1.0-py2.py3-none-any.whl (302 kB)\n","\u001b[K     |████████████████████████████████| 302 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (1.15.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (1.19.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (1.4.1)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.7.0\n","    Uninstalling keras-2.7.0:\n","      Successfully uninstalled keras-2.7.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires keras<2.8,>=2.7.0rc0, but you have keras 2.1.0 which is incompatible.\u001b[0m\n","Successfully installed keras-2.1.0\n","Requirement already satisfied: keras==2.1.0 in /usr/local/lib/python3.7/dist-packages (2.1.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (1.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (1.19.5)\n","Collecting scikit-image==0.16.2\n","  Downloading scikit_image-0.16.2-cp37-cp37m-manylinux1_x86_64.whl (26.5 MB)\n","\u001b[K     |████████████████████████████████| 26.5 MB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (2.4.1)\n","Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (1.4.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (7.1.2)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (1.2.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (3.2.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (2.6.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio>=2.3.0->scikit-image==0.16.2) (1.19.5)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (1.3.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (1.15.0)\n","Installing collected packages: scikit-image\n","  Attempting uninstall: scikit-image\n","    Found existing installation: scikit-image 0.18.3\n","    Uninstalling scikit-image-0.18.3:\n","      Successfully uninstalled scikit-image-0.18.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed scikit-image-0.16.2\n","TensorFlow 1.x selected.\n"]}]},{"cell_type":"markdown","metadata":{"id":"J-2oItL5B_T0"},"source":["##Import all the necessary Code & set Required Parameters\n","\n",">We now import all the necessary code and classes that we would need for this model to work. We will also set all the paths that might be needed for loading data from the storage.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"CC8MRbHp_Qqc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643826539332,"user_tz":-300,"elapsed":7628,"user":{"displayName":"usama shahid","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03571715957292202472"}},"outputId":"95bd45b9-fdd7-42d1-9f41-4d4ea703a72e"},"source":["import os\n","\n","# Root directory of the project\n","ROOT_DIR='/content/drive/MyDrive/FiverrOrders/22_01_05Munazza'\n","MODEL_DIR_WEIGHTS=os.path.join(ROOT_DIR,\"Weights\")\n","OUTPUT_DIR=os.path.join(ROOT_DIR,\"Outputs\")\n","%cd $ROOT_DIR\n","import imgaug.augmenters as iaa\n","import csv\n","import copy\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import mrcnn.utils as utils\n","from mrcnn import visualize\n","\n","##uncomment the following lines if not running on google colab\n","#import mrcnn.model as modellib\n","#MODEL_LOG_DIR=ROOT_DIR+\"/logs/\"\n","\n","##uncomment the following lines if running on google colab\n","import mrcnn_colab.model as modellib\n","MODEL_LOG_DIR=\"/content/\"\n","\n","from mrcnn.config import Config\n","import mrcnn.visualize\n","from mrcnn.model import log\n","import sys\n","import random\n","import math\n","import re\n","import time\n","import numpy as np\n","import cv2\n","from os.path import join\n","import sys\n","import skimage\n","from skimage.filters import threshold_mean\n","from skimage.color import rgb2gray, gray2rgb\n","import json\n","from PIL import Image, ImageDraw\n","from pathlib import Path\n","import collections\n","from collections import defaultdict\n","from pycocotools import mask as MASK\n","from skimage import measure\n","from sklearn.metrics import classification_report\n","from IPython.display import clear_output\n","from mrcnn.visualize import display_images\n","import tensorflow as tf\n","from sklearn.utils import shuffle\n","%matplotlib inline \n","# RESULTANTDATASETFOLDER=ROOT_DIR+\"/dataset\"\n","# R_Images=RESULTANTDATASETFOLDER+\"/images/\"\n","# R_Masks=RESULTANTDATASETFOLDER+\"/masks/\"\n","\n","\n","\n","def get_ax(rows=1, cols=1, size=8):\n","    \"\"\"Return a Matplotlib Axes array to be used in\n","    all visualizations in the notebook. Provide a\n","    central point to control graph sizes.\n","    \n","    Change the default size attribute to control the size\n","    of rendered images\n","    \"\"\"\n","    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n","    return ax\n","def pltGraph(epochs,history):\n","  plt.figure(figsize=(21,11))\n","  plt.subplot(231)\n","  plt.plot(epochs, history[\"loss\"], label=\"Train loss\")\n","  plt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\n","  plt.legend()\n","  plt.subplot(232)\n","  plt.plot(epochs, history[\"rpn_class_loss\"], label=\"Train RPN class ce\")\n","  plt.plot(epochs, history[\"val_rpn_class_loss\"], label=\"Valid RPN class ce\")\n","  plt.legend()  \n","  plt.subplot(233)\n","  plt.plot(epochs, history[\"rpn_bbox_loss\"], label=\"Train RPN box loss\")\n","  plt.plot(epochs, history[\"val_rpn_bbox_loss\"], label=\"Valid RPN box loss\")\n","  plt.legend()\n","  plt.subplot(234)\n","  plt.plot(epochs, history[\"mrcnn_class_loss\"], label=\"Train MRCNN class ce\")\n","  plt.plot(epochs, history[\"val_mrcnn_class_loss\"], label=\"Valid MRCNN class ce\")\n","  plt.legend()\n","  plt.subplot(235)\n","  plt.plot(epochs, history[\"mrcnn_bbox_loss\"], label=\"Train MRCNN box loss\")\n","  plt.plot(epochs, history[\"val_mrcnn_bbox_loss\"], label=\"Valid MRCNN box loss\")\n","  plt.legend()\n","  plt.subplot(236)\n","  plt.plot(epochs, history[\"mrcnn_mask_loss\"], label=\"Train Mask loss\")\n","  plt.plot(epochs, history[\"val_mrcnn_mask_loss\"], label=\"Valid Mask loss\")\n","  plt.legend()\n","  plt.show()\n","from pandas import DataFrame\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","from matplotlib.collections import QuadMesh\n","import seaborn as sn\n","from sklearn.metrics import confusion_matrix\n","from pandas import DataFrame\n","from string import ascii_uppercase\n","\n","\n","#function to calculate IoU\n","def get_iou(a, b, epsilon=1e-5):\n","    \"\"\" \n","    Given two boxes `a` and `b` defined as a list of four numbers:\n","            [x1,y1,x2,y2]\n","        where:\n","            x1,y1 represent the upper left corner\n","            x2,y2 represent the lower right corner\n","        It returns the Intersect of Union score for these two boxes.\n","    Args: \n","        a:          (list of 4 numbers) [x1,y1,x2,y2]\n","        b:          (list of 4 numbers) [x1,y1,x2,y2]\n","        epsilon:    (float) Small value to prevent division by zero\n","    Returns:\n","        (float) The Intersect of Union score.\n","    \"\"\"\n","    # COORDINATES OF THE INTERSECTION BOX\n","    x1 = max(a[0], b[0])\n","    y1 = max(a[1], b[1])\n","    x2 = min(a[2], b[2])\n","    y2 = min(a[3], b[3])\n","\n","    # AREA OF OVERLAP - Area where the boxes intersect\n","    width = (x2 - x1)\n","    height = (y2 - y1)\n","    # handle case where there is NO overlap\n","    if (width<0) or (height <0):\n","        return 0.0\n","    area_overlap = width * height\n","\n","    # COMBINED AREA\n","    area_a = (a[2] - a[0]) * (a[3] - a[1])\n","    area_b = (b[2] - b[0]) * (b[3] - b[1])\n","    area_combined = area_a + area_b - area_overlap\n","\n","    # RATIO OF AREA OF OVERLAP OVER COMBINED AREA\n","    iou = area_overlap / (area_combined+epsilon)\n","    return iou\n","\n","def gt_pred_lists(gt_class_ids, gt_bboxes, pred_class_ids, pred_bboxes, iou_tresh = 0.5):\n","\n","    \"\"\" \n","        Given a list of ground truth and predicted classes and their boxes, \n","        this function associates the predicted classes to their gt classes using a given Iou (Iou>= 0.5 for example) and returns \n","        two normalized lists of len = N containing the gt and predicted classes, \n","        filling the non-predicted and miss-predicted classes by the background class (index 0).\n","        Args    :\n","            gt_class_ids   :    list of gt classes of size N1\n","            pred_class_ids :    list of predicted classes of size N2\n","            gt_bboxes      :    list of gt boxes [N1, (x1, y1, x2, y2)]\n","            pred_bboxes    :    list of pred boxes [N2, (x1, y1, x2, y2)]\n","            \n","        Returns : \n","            gt             :    list of size N\n","            pred           :    list of size N \n","    \"\"\"\n","\n","    #dict containing the state of each gt and predicted class (0 : not associated to any other class, 1 : associated to a classe)\n","    gt_class_ids_ = {'state' : [0*i for i in range(len(gt_class_ids))], \"gt_class_ids\":list(gt_class_ids)}\n","    pred_class_ids_ = {'state' : [0*i for i in range(len(pred_class_ids))], \"pred_class_ids\":list(pred_class_ids)}\n","\n","    #the two lists to be returned\n","    pred=[]\n","    gt=[]\n","\n","    for i, gt_class in enumerate(gt_class_ids_[\"gt_class_ids\"]):\n","        for j, pred_class in enumerate(pred_class_ids_['pred_class_ids']): \n","            #check if the gt object is overlapping with a predicted object\n","            if get_iou(gt_bboxes[i], pred_bboxes[j])>=iou_tresh:\n","                #change the state of the gt and predicted class when an overlapping is found\n","                gt_class_ids_['state'][i] = 1\n","                pred_class_ids_['state'][j] = 1\n","                #chack if the overlapping objects are from the same class\n","                if (gt_class == pred_class):\n","                    gt.append(gt_class)\n","                    pred.append(pred_class)\n","                #if the overlapping objects are not from the same class \n","                else : \n","                    gt.append(gt_class)\n","                    pred.append(pred_class)\n","    #look for objects that are not predicted (gt objects that dont exists in pred objects)\n","    for i, gt_class in enumerate(gt_class_ids_[\"gt_class_ids\"]):\n","        if gt_class_ids_['state'][i] == 0:\n","            gt.append(gt_class)\n","            pred.append(0)\n","    #look for objects that are mispredicted (pred objects that dont exists in gt objects)\n","    for j, pred_class in enumerate(pred_class_ids_[\"pred_class_ids\"]):\n","        if pred_class_ids_['state'][j] == 0:\n","            gt.append(0)\n","            pred.append(pred_class)\n","    return gt, pred\n","\n","def plot_confusion_matrix_from_data(y_test, predictions, columns=None, annot=True, cmap=\"Oranges\",\n","      fmt='.2f', fz=11, lw=0.5, cbar=False, figsize=[36,36], show_null_values=0, pred_val_axis='lin'):\n","    \"\"\"\n","        plot confusion matrix function with y_test (actual values) and predictions,\n","        return the tp, fp and fn\n","    \"\"\"\n","\n","    #data\n","    if(not columns):\n","        columns = ['class %s' %(i) for i in list(ascii_uppercase)[0:max(len(np.unique(y_test)),len(np.unique(predictions)))]]\n","    \n","    y_test = np.array(y_test)\n","    predictions = np.array(predictions)\n","    #confusion matrix \n","    confm = confusion_matrix(y_test, predictions)\n","    num_classes = len(columns)\n","    sn.heatmap(confm, annot = True)\n","    #compute tp fn fp\n","    fp=[0]*num_classes\n","    fn=[0]*num_classes\n","    tp=[0]*num_classes\n","    tn = [0]*num_classes\n","    tn[0] = confm[1][1]\n","    tn[1] = confm[0][0]\n","    for i in range(confm.shape[0]):\n","        fp[i]+=np.sum(confm[i])-np.diag(confm)[i]\n","        fn[i]+=np.sum(np.transpose(confm)[i])-np.diag(confm)[i]\n","        for j in range(confm.shape[1]):\n","            if i==j:\n","                tp[i]+=confm[i][j]\n","    return tp, fp, fn, tn, confm\n","# Compute VOC-style Average Precision\n","\n","\n","\n","def compute_batch_ap(dataset,ids,limit):\n","    \n","    shuffle(ids,random_state=0)\n","    APs = []\n","    AllPricisions=[]\n","    AllRecalls=[]\n","    AllOverlaps=[]\n","\n","    counter=0\n","    #ground-truth and predictions lists for each object\n","    gt_tot = np.array([])\n","    pred_tot = np.array([])\n","    for image_id in image_ids:\n","        # Load image\n","        clear_output(wait=True)\n","        print(counter)\n","       \n","        ErrorFlag=True\n","        # while ErrorFlag:\n","        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","              modellib.load_image_gt(dataset, config,\n","                                   image_id, use_mini_mask=False)\n","          # Run object detection\n","        results = model.detect([image], verbose=0)\n","        r = results[0]\n","        try:\n","            #compute gt_tot and pred_tot\n","            gt, pred = gt_pred_lists(gt_class_id, gt_bbox, r['class_ids'], r['rois'])\n","            gt_tot = np.append(gt_tot, gt)\n","            pred_tot = np.append(pred_tot, pred)\n","    \n","            # Compute AP\n","            AP, precisions, Recalls, overlaps =\\\n","                utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n","                              r['rois'], r['class_ids'], r['scores'], r['masks'])\n","            ErrorFlag=False\n","        except:\n","            # image_id = random.choice(ids)\n","            # ErrorFlag=True\n","            continue\n","        APs.append(AP)\n","        AllPricisions.append(precisions)\n","        AllRecalls.append(Recalls)\n","        AllOverlaps.append(overlaps)\n","        counter=counter+1\n","        if counter>=limit:\n","          break\n","\n","    return APs,AllPricisions,AllRecalls,AllOverlaps,gt_tot,pred_tot\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/FiverrOrders/22_01_05Munazza\n"]},{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]}]},{"cell_type":"markdown","metadata":{"id":"noju9uak_Qqc"},"source":["##Define Configurations.\n","\n",">We now create a CavityConfig class that will inherit the Config class from MRCNN. It will hold all the configuration that are necessary for designing the architechture of our model for training. Similarly we create class CavityInferenceConfig that inherits CavityTrainingConfig. This class will hold all the configurations that might be needed to create the architechture of model for inference/detections.\n","\n"]},{"cell_type":"code","metadata":{"id":"AL_qSNAd_Qqc"},"source":["class MappingTrainingConfig(Config):\n","    \"\"\"Configuration for training on the box_synthetic dataset.\n","    Derives from the base Config class and overrides specific values.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"MappingTrainingModel\"\n","\n","    # Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 1  # background + 1 for building\n","    IMAGE_SHAPE = [512, 512, 3]\n","    IMAGE_MIN_DIM = 512\n","    IMAGE_MAX_DIM = 512\n","\n","    # You can experiment with this number to see if it improves training\n","    STEPS_PER_EPOCH = 100\n","\n","    # This is how often validation is run. If you are using too much hard drive space\n","    # on saved models (in the MODEL_DIR), try making this value larger.\n","    VALIDATION_STEPS = 50\n","    MINI_MASK_SHAPE=(112, 112)\n","    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n","    # BACKBONE = 'resnet50'\n","\n","    # To be honest, I haven't taken the time to figure out what these do\n","    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n","    TRAIN_ROIS_PER_IMAGE = 32\n","    MAX_GT_INSTANCES = 50 \n","    POST_NMS_ROIS_INFERENCE = 500 \n","    POST_NMS_ROIS_TRAINING = 1000\n","    def __init__(self):\n","        super().__init__()\n","\n","class MappingInferenceConfig(MappingTrainingConfig):\n","    NAME = \"MappingInferenceModel\"\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","    DETECTION_MIN_CONFIDENCE=0.65\n","    def __init__(self):\n","        super().__init__()\n","    \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kj506YQ-_Qqc"},"source":["##Define Dataset.\n","\n",">We now create a CocoLikeDataset class that will inherit the Utils.Dataset class from MRCNN. It will hold all the methods that are necessary for using our dataset with our model.\n","\n"]},{"cell_type":"code","metadata":{"id":"SUwRkn4B_Qqd"},"source":["\n","class Dataset(mrcnn.utils.Dataset):\n","    \n","    def load_data(self,FileNames,imagesDir,maskDir):\n","\n","        # Add classes\n","        self.add_class(\"Building\", 1, \"Building\")\n","        \n","        count = 0\n","        for FileName in FileNames:\n","          imagePath=os.path.join(imagesDir,FileName)\n","          maskPath=os.path.join(maskDir,FileName)\n","          if os.path.exists(maskPath) and os.path.exists(imagePath):\n","                    self.add_image(\"Building\",\n","                                   image_id=count,\n","                                   path=imagePath,\n","                                   maskpath=maskPath\n","                                   )\n","                    count += 1\n","    \n","    def load_mask(self, image_id):\n","        info = self.image_info[image_id]\n","        \n","        path_mask = info['maskpath']\n","        image_mask = skimage.io.imread(path_mask)\n","        \n","        im_bw = cv2.cvtColor(image_mask, cv2.COLOR_RGB2GRAY)\n","\n","        (thresh, im_bw) = cv2.threshold(im_bw, 1, 255, 0)\n","        \n","        contours, hierarchy = cv2.findContours(im_bw, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","\n","\n","\n","        #image = gray2rgb(image)\n","\n","        # ret, thresh = cv2.threshold(image_mask, 0, 255, 0)\n","        # contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","        \n","\n","\n","\n","        class_ids = np.ones(len(contours), np.int32)\n","        \n","        masks = np.zeros((image_mask.shape[0],image_mask.shape[1], len(contours)), dtype=np.bool)\n","        \n","        for i in range(len(contours)):\n","            img_cont = np.zeros((image_mask.shape[0],image_mask.shape[1],3) , dtype=np.int8)\n","            \n","            cv2.drawContours(img_cont, contours, i, (255,255,255), -1)\n","\n","            bimask = np.zeros(img_cont.shape[0:2], dtype=np.uint8)\n","\n","            bimask = img_cont[:,:, 0]\n","            \n","            thresh = threshold_mean(bimask)\n","            mask = bimask > thresh\n","            masks[:, :, i]= mask\n","\n","            \n","        return masks, class_ids   \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6A6csih_Qqd"},"source":["##Define Annotation Processor.\n","\n",">We now create a DatasetProcessor class that will help us manipulate the COCO annotations File. It will hold all the methods that are necessary for manipulating coco Annotation file. We will be using this class to manipulate annotation files during the process of Increamental Learning.\n","\n"]},{"cell_type":"code","metadata":{"id":"r-jdWETX_Qqe"},"source":["class DatasetProcessor:\n","    def __init__(self):\n","        self.jsonFile={}\n","        self.annotationFilePath=\"\"\n","    def getAnnotationsDictByImageId(self):\n","      annotations ={}\n","      for annotation in self.jsonFile['annotations']:\n","            image_id = annotation['image_id']\n","            if image_id not in annotations:\n","                annotations[image_id] = []\n","            annotations[image_id].append(annotation)\n","      return annotations\n","\n","    def readJsonFile(self, annotation_json):\n","        #reading all the json data into a cocojson variable\n","        json_file = open(annotation_json)\n","        coco_json = json.load(json_file)\n","        json_file.close()\n","        # storing it into a class variable so it can be used later\n","        # we have to keep all the data so that we don't change any format of the data\n","        self.jsonFile=coco_json\n","        self.annotationFilePath=annotation_json\n","        self.updateCurrentMaxId()\n","        \n","    def updateCurrentMaxId(self):\n","        # set the current max Ids which can be help full for adding images later\n","        self.maxAnnotationId=-1\n","        self.maxImageId=-1\n","        for annotation in self.jsonFile['annotations']:\n","            id=int(annotation['id'])\n","            if (id>self.maxAnnotationId):\n","              self.maxAnnotationId=id\n","        for image in self.jsonFile['images']:\n","            image_id = int(image['id'])\n","            if (image_id>self.maxImageId):\n","              self.maxImageId=image_id\n","        \n","\n","\n","    def RemoveUnanotatedImages(self):\n","        # reading all the annotations\n","        \n","        annotations ={}\n","        for annotation in self.jsonFile['annotations']:\n","            image_id = annotation['image_id']\n","            if image_id not in annotations:\n","                annotations[image_id] = []\n","            annotations[image_id].append(annotation)\n","        seen_images = {}\n","        #read all the images from json file and append only the image that does not repeat and has an annotation in annotations\n","        images=[]\n","        for image in self.jsonFile['images']:\n","            image_id = image['id']\n","            if image_id not in seen_images:\n","                seen_images[image_id] = image\n","                image_annotations = annotations.get(image_id,None)\n","                if image_annotations!=None and image_annotations!=[]:\n","                  images.append(image)\n","                  \n","        # replace all the images of the file with the updated list of images\n","        self.jsonFile['images']=images\n","    def WriteAnnotationFile(self,savePath):\n","        directory,fileName=os.path.split(savePath)\n","        saveLocation=os.path.join(directory,fileName)\n","        if not os.path.isdir(directory):\n","          os.makedirs(directory)\n","          \n","        with open(saveLocation, 'w') as outfile:\n","          json.dump(self.jsonFile, outfile)\n","\n","\n","    def checkIfImageExists(self,imageFileName):\n","        # this method checks the existence of the image. if it exist then it returns its id with a True else it return false with none\n","        for image in self.jsonFile['images']:\n","            if image['file_name']==imageFileName:\n","              return True,image      \n","        return False,None\n","    def addImagesFromDirExceptGivenList(self,dirPath,expnameList):\n","        #this method will add all the images from dirpath except the given name list\n","        #this will also return the image with the details about it in json file\n","        for f in os.listdir(dirPath):\n","          if f not in expnameList and os.path.isfile(os.path.join(dirPath, f)):\n","            image=cv2.imread(os.path.join(dirPath, f))\n","            self.addImage(f,image)\n","        \n","        \n","\n","    def RemoveAnnotationsByImageId(self,image_id):\n","        # this method removes any annotations of an image that might exist by image Id\n","        annotations=[]\n","        for annotation in self.jsonFile['annotations']:\n","            if annotation['image_id']!=image_id:\n","              annotations.append(annotation)\n","        self.jsonFile['annotations']=annotations\n","    \n","    def RemoveImageById(self,image_id):\n","        images=[]\n","        for image in self.jsonFile['images']:\n","            if image['id']!=image_id:\n","              images.append(image)\n","            else :\n","              self.RemoveAnnotationsByImageId(image['id'])\n","        self.jsonFile['images']=images\n","    \n","    \n","    def RemoveImageByName(self,name):\n","        images=[]\n","        for image in self.jsonFile['images']:\n","            if image['file_name']!=name:\n","              images.append(image)\n","            else :\n","              self.RemoveAnnotationsByImageId(image['id'])\n","        self.jsonFile['images']=images\n","\n","    def removeAllImages(self):\n","      self.jsonFile['images']=[]\n","    \n","    def removeAllAnnotations(self):\n","      self.jsonFile['annotations']=[]\n","\n","    def addImage(self,imageFileName,image): \n","        # method to add a image to our current json file\n","        #first we check if image already exist. if it does not we add new image\n","        # if it does then we empty it annotations\n","\n","        # check if image exists \n","        found,im=self.checkIfImageExists(imageFileName)\n","        if not found:\n","        # if not found use assign new id and add the image\n","          self.maxImageId=self.maxImageId+1\n","          id=self.maxImageId\n","          im={\n","             'file_name':imageFileName,\n","             'id':id,\n","             'height':image.shape[0],\n","             'width':image.shape[1]\n","             }\n","          self.jsonFile['images'].append(im)\n","        else :\n","          # else remove the current annotations for the image\n","          self.RemoveAnnotationsByImageId(id)\n","        return im\n","\n","    def addImageAnnotations(self,r_masks,categoryIds,imageFileName,image):\n","        # adding this image to annotations file \n","        im=self.addImage(imageFileName,image)\n","        # adding annotations of each instance present in this image.\n","        for i in range(r_masks.shape[2]):\n","          self.addImageAnnotation(im['id'],r_masks[:,:,i],categoryIds[i])\n","\n","         \n","    def addImageAnnotation(self,image_id,r_mask,categoryId):\n","         # translating the mask in to polygons\n","         temp_mask = r_mask.astype(int)\n","         ground_truth_binary_mask = np.array(temp_mask, dtype=np.uint8)\n","         fortran_ground_truth_binary_mask = np.asfortranarray(ground_truth_binary_mask)\n","         encoded_ground_truth = MASK.encode(fortran_ground_truth_binary_mask)\n","         ground_truth_area = MASK.area(encoded_ground_truth)\n","         ground_truth_bounding_box = MASK.toBbox(encoded_ground_truth)\n","         contours = measure.find_contours(ground_truth_binary_mask, 0.5)\n","         # adding annotation for this mask in to the annotation file\n","         self.maxAnnotationId=self.maxAnnotationId+1\n","         annotation = {\n","             \"segmentation\": [],\n","             \"area\": int(ground_truth_area.tolist()),\n","             \"iscrowd\": 0,\n","             \"image_id\": int(image_id),\n","             \"bbox\": [int(x) for x in ground_truth_bounding_box.tolist()],\n","             \"category_id\": int(categoryId),\n","             \"id\": int(self.maxAnnotationId)\n","         }\n","\n","            # putting each contour as a segmentation in our annotation for this mask\n","         for contour in contours:\n","             contour = np.flip(contour, axis=1)\n","             segmentation = contour.ravel().tolist()\n","             annotation[\"segmentation\"].append(segmentation)\n","         self.jsonFile['annotations'].append(annotation)\n","         \n","    def BreakDataset(self,TrainRatio):\n","      annotations={}\n","      for annotation in self.jsonFile['annotations']:\n","            image_id = annotation['image_id']\n","            if image_id not in annotations:\n","                annotations[image_id] = []\n","            annotations[image_id].append(annotation)\n","      random.shuffle(self.jsonFile['images'])\n","      devider=int(TrainRatio*len(self.jsonFile['images']))\n","      \n","      trainImages=self.jsonFile['images'][:devider]\n","      \n","      testImages=self.jsonFile['images'][devider:]\n","      \n","      TrainFile=DatasetProcessor()\n","      TrainFile.jsonFile=self.jsonFile.copy()\n","      TrainFile.removeAllAnnotations()\n","      TrainFile.removeAllImages()\n","      \n","      TestFile=DatasetProcessor()\n","      TestFile.jsonFile=self.jsonFile.copy()\n","      TestFile.removeAllAnnotations()\n","      TestFile.removeAllImages()\n","      \n","      for image in trainImages:\n","           TrainFile.jsonFile['images'].append(image)\n","           TrainFile.jsonFile['annotations'] = TrainFile.jsonFile['annotations'] + annotations.get(image['id'],[])\n","      \n","      for image in testImages:\n","           TestFile.jsonFile['images'].append(image)\n","           TestFile.jsonFile['annotations'] = TestFile.jsonFile['annotations'] + annotations.get(image['id'],[])\n","     \n","      return TrainFile,TestFile\n","\n","    def CombineCOCOFiles(DatasetProcessorsList):\n","      \"\"\"takes a list of datasetProcessor Objects and combines the images and annotations and returns it\"\"\"\n","      \"\"\"\"please make sure that all the files that you are trying to combine have same categories Other wise you will run into errors\"\"\"\n","      for elem in DatasetProcessorsList:\n","        assert elem.jsonFile['categories']==DatasetProcessorsList[0].jsonFile['categories'] , \"all the files should have same categories\"\n","      \n","      CombinedFile=DatasetProcessor()\n","      CombinedFile.jsonFile=DatasetProcessorsList[0].jsonFile.copy()\n","      CombinedFile.removeAllAnnotations()\n","      CombinedFile.removeAllImages()\n","      CombinedFile.updateCurrentMaxId()\n","      for File in DatasetProcessorsList:\n","        annotationsDict=File.getAnnotationsDictByImageId()\n","        for image in File.jsonFile['images']:\n","          CombinedFile.maxImageId=CombinedFile.maxImageId+1\n","          ImageAnnotations=annotationsDict.get(image['id'],[])\n","          image['id']=CombinedFile.maxImageId\n","          CombinedFile.jsonFile['images'].append(image)\n","          for annotation in ImageAnnotations:\n","            CombinedFile.maxAnnotationId=CombinedFile.maxAnnotationId+1\n","            annotation['image_id']=image['id']\n","            annotation['id']=CombinedFile.maxAnnotationId\n","            CombinedFile.jsonFile['annotations'].append(annotation)\n","      return CombinedFile\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Patching related work"],"metadata":{"id":"EIGx1ATUYaAS"}},{"cell_type":"code","source":["def makeWindows(image,height,width):\n","  img=resizeToFit(image,height,width)\n","  n_x=int(img.shape[1]/width)\n","  n_y=int(img.shape[0]/height)\n","  \n","  images_dict={}\n","  for y in range(1,n_y+1):\n","    for x in range(1,n_x+1):\n","      images_dict[(x,y)]=img[height*(y-1):height*y,width*(x-1):width*x,:]\n","  return images_dict, img\n","\n","# we redifine our visualize instances to match our current requirements\n","\n","def resize(image, width, height):\n","        '''\n","        Resize PIL image keeping ratio and using black background.\n","        '''\n","        from PIL import Image\n","        im = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image_pil=Image.fromarray(im)\n","        ratio_w = width / image_pil.width\n","        ratio_h = height / image_pil.height\n","        if ratio_w < ratio_h:\n","            # It must be fixed by width\n","            resize_width = width\n","            resize_height = round(ratio_w * image_pil.height)\n","        else:\n","            # Fixed by height\n","            resize_width = round(ratio_h * image_pil.width)\n","            resize_height = height\n","        image_resize = image_pil.resize((resize_width, resize_height), Image.ANTIALIAS)\n","        background = Image.new('RGBA', (width, height), (0, 0, 0, 1))\n","        offset = (round((width - resize_width) / 2), round((height - resize_height) / 2))\n","        background.paste(image_resize, offset)\n","        return np.asarray(background.convert('RGB'))\n","\n","def resizeToFit(image,width,height):\n","    originalWidthRatio=math.ceil(image.shape[1]/width)\n","    originalHeightRatio=math.ceil(image.shape[0]/height)\n","    \n","    img=resize(image,originalWidthRatio*width,originalHeightRatio*height)\n","    return img\n","def MergeMasks(masks):\n","    if masks.shape[2]==1:\n","        return masks[:,:,0] \n","    if masks.shape[2]==0:\n","        return np.zeros((masks.shape[0],masks.shape[1]),dtype=np.bool)\n","    \n","    msk=masks[:,:,0]\n","    for i in range(1,masks.shape[2]):\n","        msk=np.logical_or(msk,masks[:,:,i]) \n","    return msk\n","    \n","\n","def processPatches(img,model,path,Patch_height,Patch_width, showPatches=False):\n","  # imageName=os.path.split(path)[1][:-4]\n","  # dir=PREDICT_OUT_DIR+\"/\"+imageName\n","  # if not os.path.exists(dir):\n","  #   os.mkdir(dir)\n","  images,img=makeWindows(img,Patch_height,Patch_width)\n","  print(\"image has been devided into \"+str(len(images))+\" sections\")\n","\n","\n","  total_no_b=0\n","  outputDict={}\n","  outputMasksDict={}\n","  counter=0\n","  for i,j in images:\n","    print(str(counter)+\" out of \"+str(len(images))+\" patches done \")\n","    counter=counter+1\n","    image=images[(i,j)]\n","    # Run object detection\n","    results = model.detect([image])\n","    # Display results\n","    r = results[0]\n","    no_b=len(r['class_ids'])\n","    total_no_b=total_no_b+no_b\n","\n","    # a=place_instances(image, r['rois'], r['masks'], r['class_ids'], \n","    #                         ['BG','Building'], r['scores'],\n","    #                         title=\"Number of buildings in this image:\"+str(no_b),show=showPatches)\n","    croppedMask=MergeMasks(r['masks'])\n","    \n","    if showPatches:\n","        display_instances(image, r['rois'], r['masks'], r['class_ids'], ['BG','Building'], r['scores'], title=str(i)+\" \"+str(j))\n","        plt.show()\n","    # if showPatches:\n","    #     croppedMaskArr=np.zeros((croppedMask.shape[0],croppedMask.shape[1],1),dtype=np.bool)\n","    #     croppedMaskArr[:,:,0]=croppedMask\n","    #     display_instances(image,  np.array([[0,0,image.shape[0],image.shape[1]]]),croppedMaskArr, np.array([1]), ['BG','Building'], np.array([1]), title=str(i)+\" \"+str(j))\n","    #     plt.show()\n","    #save to drive as image\n","    # savepath=dir+\"/\"+str(i)+\"_\"+str(j)+\"Detections.jpg\"\n","    # saveImg=cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n","    # cv2.imwrite(savepath,saveImg)  \n","    # savepath=dir+\"/\"+str(i)+\"_\"+str(j)+\"Original.jpg\"\n","    # cv2.imwrite(savepath,image)\n","    # outputDict[(i,j)]=a\n","    outputMasksDict[(i,j)]=croppedMask\n","\n","  # combine all patches again  \n","#   outputImg=np.zeros_like(img)\n","  outputMask=np.zeros((img.shape[0],img.shape[1]),dtype=np.bool)\n","  # for x,y in outputDict:\n","  #     outputImg[Patch_height*(y-1):Patch_height*y,Patch_width*(x-1):Patch_width*x,:]=outputDict[(x,y)]\n","  for x,y in outputMasksDict:\n","      outputMask[Patch_height*(y-1):Patch_height*y,Patch_width*(x-1):Patch_width*x]=outputMasksDict[(x,y)]\n","    \n","  # #  save output image\n","  # savepath=dir+\"/~ConcatinatedImage.tif\"\n","  # saveImg=cv2.cvtColor(outputImg, cv2.COLOR_BGR2RGB)\n","  # cv2.imwrite(savepath,saveImg)\n","  # savepath=dir+\"/~OriginalImage.tif\"\n","  # saveImg=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","  # cv2.imwrite(savepath,saveImg)\n","  \n","  maskArr=np.zeros((outputMask.shape[0],outputMask.shape[1],1),dtype=np.bool)\n","  maskArr[:,:,0]=outputMask\n","  return img, maskArr,  total_no_b\n","def loadMaskFromFile(path,Patch_height,Patch_width):\n","    mask=cv2.imread(path)\n","    mask=resizeToFit(mask,Patch_height,Patch_width)\n","    bnMask=np.zeros((mask.shape[0],mask.shape[1],1),dtype=np.bool)\n","    bnMask[:,:,0]=mask[:,:,0]>0\n","    return bnMask\n","def SaveMaskToFile(path,mask):\n","    mskImg=(mask[:,:,0]*255).astype(np.uint8)\n","    mskrgbImg=np.zeros((mskImg.shape[0],mskImg.shape[1],3),dtype=np.uint8)\n","    mskrgbImg[:,:,0]=mskImg\n","    mskrgbImg[:,:,1]=mskImg\n","    mskrgbImg[:,:,2]=mskImg\n","    cv2.imwrite(path,mskrgbImg)\n","\n","def RemoveLowSizedInstances(msk, minArea):\n","    mskImg=(msk[:,:,0]*255).astype(np.uint8)\n","    contours = cv2.findContours(mskImg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    contours = contours[0] if len(contours) == 2 else contours[1]\n","    \n","    mask = np.zeros_like(mskImg)\n","    \n","    for cnt in contours:\n","        if cv2.contourArea(cnt)>minArea:\n","            cv2.fillPoly(mask, pts = [cnt], color =(255,))\n","    maskarr=np.zeros_like(msk)\n","    maskarr[:,:,0]=mask>0\n","    return maskarr"],"metadata":{"id":"IWECKyhjYWOs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tr0oIRxFxbki"},"source":["###Define Visualizer"]},{"cell_type":"code","metadata":{"id":"k3dCAPWexfWg"},"source":["import colorsys\n","from skimage.measure import find_contours\n","\n","from matplotlib import patches,  lines\n","from matplotlib.patches import Polygon\n","def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n","                   interpolation=None):\n","    \"\"\"Display the given set of images, optionally with titles.\n","    images: list or array of image tensors in HWC format.\n","    titles: optional. A list of titles to display with each image.\n","    cols: number of images per row\n","    cmap: Optional. Color map to use. For example, \"Blues\".\n","    norm: Optional. A Normalize instance to map values to colors.\n","    interpolation: Optional. Image interpolation to use for display.\n","    \"\"\"\n","    titles = titles if titles is not None else [\"\"] * len(images)\n","    rows = len(images) // cols + 1\n","    plt.figure(figsize=(14, 14 * rows // cols))\n","    i = 1\n","    for image, title in zip(images, titles):\n","        plt.subplot(rows, cols, i)\n","        plt.title(title, fontsize=9)\n","        plt.axis('off')\n","        plt.imshow(image.astype(np.uint8), cmap=cmap,\n","                   norm=norm, interpolation=interpolation)\n","        i += 1\n","    plt.show()\n","def apply_mask(image, mask, color, alpha=0.5):\n","    \"\"\"Apply the given mask to the image.\n","    \"\"\"\n","    for c in range(3):\n","        image[:, :, c] = np.where(mask == 1,\n","                                  image[:, :, c] *\n","                                  (1 - alpha) + alpha * color[c] * 255,\n","                                  image[:, :, c])\n","    return image\n","\n","def random_colors(N, bright=True):\n","    \"\"\"\n","    Generate random colors.\n","    To get visually distinct colors, generate them in HSV space then\n","    convert to RGB.\n","    \"\"\"\n","    brightness = 1.0 if bright else 0.7\n","    hsv = [(i / N, 1, brightness) for i in range(N)]\n","    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n","    random.shuffle(colors)\n","    return colors\n","\n","def display_instances(image, boxes, masks, class_ids, class_names,\n","                      scores=None, title=\"\",\n","                      figsize=(16, 16), ax=None,\n","                      show_mask=True, show_bbox=True,\n","                      colors=None, captions=None):\n","    \"\"\"\n","    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n","    masks: [height, width, num_instances]\n","    class_ids: [num_instances]\n","    class_names: list of class names of the dataset\n","    scores: (optional) confidence scores for each box\n","    title: (optional) Figure title\n","    show_mask, show_bbox: To show masks and bounding boxes or not\n","    figsize: (optional) the size of the image\n","    colors: (optional) An array or colors to use with each object\n","    captions: (optional) A list of strings to use as captions for each object\n","    \"\"\"\n","    # Number of instances\n","    N = boxes.shape[0]\n","    if not N:\n","        print(\"\\n*** No instances to display *** \\n\")\n","    else:\n","        assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n","\n","    # If no axis is passed, create one and automatically call show()\n","    auto_show = False\n","    if not ax:\n","        _, ax = plt.subplots(1, figsize=figsize)\n","        auto_show = True\n","\n","    # Generate random colors\n","    colors = colors or random_colors(N)\n","\n","    # Show area outside image boundaries.\n","    height, width = image.shape[:2]\n","    ax.set_ylim(height + 10, -10)\n","    ax.set_xlim(-10, width + 10)\n","    ax.axis('off')\n","    ax.set_title(title)\n","\n","    masked_image = image.astype(np.uint32).copy()\n","    for i in range(N):\n","        color = colors[i]\n","\n","        # Bounding box\n","        if not np.any(boxes[i]):\n","            # Skip this instance. Has no bbox. Likely lost in image cropping.\n","            continue\n","        y1, x1, y2, x2 = boxes[i]\n","        if show_bbox:\n","            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n","                                alpha=0.7, linestyle=\"dashed\",\n","                                edgecolor=color, facecolor='none')\n","            ax.add_patch(p)\n","\n","        # Label\n","        if not captions:\n","            class_id = class_ids[i]\n","            score = scores[i] if scores is not None else None\n","            label = class_names[class_id]\n","            caption = \"{} {:.3f}\".format(label, score) if score else label\n","        else:\n","            caption = captions[i]\n","        ax.text(x1, y1 + 8, caption,\n","                color='w', size=11, backgroundcolor=\"none\")\n","\n","        # Mask\n","        mask = masks[:, :, i]\n","        if show_mask:\n","            masked_image = apply_mask(masked_image, mask, color)\n","\n","        # Mask Polygon\n","        # Pad to ensure proper polygons for masks that touch image edges.\n","        padded_mask = np.zeros(\n","            (mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n","        padded_mask[1:-1, 1:-1] = mask\n","        contours = find_contours(padded_mask, 0.5)\n","        for verts in contours:\n","            # Subtract the padding and flip (y, x) to (x, y)\n","            verts = np.fliplr(verts) - 1\n","            p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n","            ax.add_patch(p)\n","    ax.imshow(masked_image.astype(np.uint8))\n","    if auto_show:\n","        pass\n","        # plt.show()\n","def display_top_masks(image, mask, class_ids, class_names, limit=4):\n","    \"\"\"Display the given image and the top few class masks.\"\"\"\n","    to_display = []\n","    titles = []\n","    to_display.append(image)\n","    titles.append(\"H x W={}x{}\".format(image.shape[0], image.shape[1]))\n","    # Pick top prominent classes in this image\n","    unique_class_ids = np.unique(class_ids)\n","    mask_area = [np.sum(mask[:, :, np.where(class_ids == i)[0]])\n","                 for i in unique_class_ids]\n","    top_ids = [v[0] for v in sorted(zip(unique_class_ids, mask_area),\n","                                    key=lambda r: r[1], reverse=True) if v[1] > 0]\n","    # Generate images and titles\n","    for i in range(limit):\n","        class_id = top_ids[i] if i < len(top_ids) else -1\n","        # Pull masks of instances belonging to the same class.\n","        m = mask[:, :, np.where(class_ids == class_id)[0]]\n","        m = np.sum(m * np.arange(1, m.shape[-1] + 1), -1)\n","        to_display.append(m)\n","        titles.append(class_names[class_id] if class_id != -1 else \"-\")\n","    display_images(to_display, titles=titles, cols=limit + 1, cmap=\"Blues_r\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Infer\n"],"metadata":{"id":"tyZqf1wO7yHr"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FwueM78YOikg","executionInfo":{"status":"ok","timestamp":1641749167968,"user_tz":-300,"elapsed":13,"user":{"displayName":"usama shahid","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03571715957292202472"}},"outputId":"1f02b566-5014-4313-f358-a0a9f7f3e789"},"source":["config = MappingInferenceConfig()\n","config.display()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Configurations:\n","BACKBONE                       resnet101\n","BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n","BATCH_SIZE                     1\n","BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n","COMPUTE_BACKBONE_SHAPE         None\n","DETECTION_MAX_INSTANCES        100\n","DETECTION_MIN_CONFIDENCE       0.65\n","DETECTION_NMS_THRESHOLD        0.3\n","FPN_CLASSIF_FC_LAYERS_SIZE     1024\n","GPU_COUNT                      1\n","GRADIENT_CLIP_NORM             5.0\n","IMAGES_PER_GPU                 1\n","IMAGE_CHANNEL_COUNT            3\n","IMAGE_MAX_DIM                  512\n","IMAGE_META_SIZE                14\n","IMAGE_MIN_DIM                  512\n","IMAGE_MIN_SCALE                0\n","IMAGE_RESIZE_MODE              square\n","IMAGE_SHAPE                    [512 512   3]\n","LEARNING_MOMENTUM              0.9\n","LEARNING_RATE                  0.001\n","LOSS_WEIGHTS                   {'rpn_class_loss': 10.0, 'rpn_bbox_loss': 10.0, 'mrcnn_class_loss': 10.0, 'mrcnn_bbox_loss': 10.0, 'mrcnn_mask_loss': 10.0}\n","MASK_POOL_SIZE                 14\n","MASK_SHAPE                     [28, 28]\n","MAX_GT_INSTANCES               50\n","MEAN_PIXEL                     [123.7 116.8 103.9]\n","MINI_MASK_SHAPE                (112, 112)\n","NAME                           MappingInferenceModel\n","NUM_CLASSES                    2\n","POOL_SIZE                      7\n","POST_NMS_ROIS_INFERENCE        500\n","POST_NMS_ROIS_TRAINING         1000\n","PRE_NMS_LIMIT                  6000\n","ROI_POSITIVE_RATIO             0.33\n","RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n","RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n","RPN_ANCHOR_STRIDE              1\n","RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n","RPN_NMS_THRESHOLD              0.7\n","RPN_TRAIN_ANCHORS_PER_IMAGE    256\n","STEPS_PER_EPOCH                100\n","TOP_DOWN_PYRAMID_SIZE          256\n","TRAIN_BN                       False\n","TRAIN_ROIS_PER_IMAGE           32\n","USE_MINI_MASK                  True\n","USE_RPN_ROIS                   True\n","VALIDATION_STEPS               50\n","WEIGHT_DECAY                   0.0001\n","\n","\n"]}]},{"cell_type":"code","metadata":{"id":"37CwLbLiPM2J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641749173495,"user_tz":-300,"elapsed":5538,"user":{"displayName":"usama shahid","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03571715957292202472"}},"outputId":"2cb766a4-eeba-4fde-9b6c-fd37043599c2"},"source":["model = modellib.MaskRCNN(mode=\"inference\", config=config,\n","                          model_dir=MODEL_LOG_DIR)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /content/drive/MyDrive/FiverrOrders/22_01_05Munazza/mrcnn_colab/model.py:342: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/drive/MyDrive/FiverrOrders/22_01_05Munazza/mrcnn_colab/model.py:400: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/MyDrive/FiverrOrders/22_01_05Munazza/mrcnn_colab/model.py:424: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","WARNING:tensorflow:From /content/drive/MyDrive/FiverrOrders/22_01_05Munazza/mrcnn_colab/model.py:724: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n","\n","WARNING:tensorflow:From /content/drive/MyDrive/FiverrOrders/22_01_05Munazza/mrcnn_colab/model.py:726: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n","\n","WARNING:tensorflow:From /content/drive/MyDrive/FiverrOrders/22_01_05Munazza/mrcnn_colab/model.py:776: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n"]}]},{"cell_type":"code","metadata":{"id":"gE5cvR7-PM2K"},"source":["path=MODEL_DIR_WEIGHTS+'/BestModelWeights1.h5'\n","model.load_weights(path, by_name=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imgpath=\"/content/drive/MyDrive/FiverrOrders/22_01_05Munazza/AlignedData/images/060102w.tif\"\n","mskpath=\"/content/drive/MyDrive/FiverrOrders/22_01_05Munazza/AlignedData/masks/060102w.tif\"\n","_, imgName=os.path.split(imgpath)\n","Patch_height=300\n","Patch_width=300 \n","img=cv2.imread(imgpath)\n","OriginalMask=loadMaskFromFile(mskpath,Patch_height,Patch_width)\n","img,PredictedMask,no_b1=processPatches(img,model,path,Patch_height,Patch_width,showPatches=False)\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUtOdXTpdDaf","executionInfo":{"status":"ok","timestamp":1641749285124,"user_tz":-300,"elapsed":52752,"user":{"displayName":"usama shahid","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03571715957292202472"}},"outputId":"bc0fb53d-fb89-43ee-bbcd-b137c9b898f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["image has been devided into 132 sections\n","0 out of 132 patches done \n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","1 out of 132 patches done \n","2 out of 132 patches done \n","3 out of 132 patches done \n","4 out of 132 patches done \n","5 out of 132 patches done \n","6 out of 132 patches done \n","7 out of 132 patches done \n","8 out of 132 patches done \n","9 out of 132 patches done \n","10 out of 132 patches done \n","11 out of 132 patches done \n","12 out of 132 patches done \n","13 out of 132 patches done \n","14 out of 132 patches done \n","15 out of 132 patches done \n","16 out of 132 patches done \n","17 out of 132 patches done \n","18 out of 132 patches done \n","19 out of 132 patches done \n","20 out of 132 patches done \n","21 out of 132 patches done \n","22 out of 132 patches done \n","23 out of 132 patches done \n","24 out of 132 patches done \n","25 out of 132 patches done \n","26 out of 132 patches done \n","27 out of 132 patches done \n","28 out of 132 patches done \n","29 out of 132 patches done \n","30 out of 132 patches done \n","31 out of 132 patches done \n","32 out of 132 patches done \n","33 out of 132 patches done \n","34 out of 132 patches done \n","35 out of 132 patches done \n","36 out of 132 patches done \n","37 out of 132 patches done \n","38 out of 132 patches done \n","39 out of 132 patches done \n","40 out of 132 patches done \n","41 out of 132 patches done \n","42 out of 132 patches done \n","43 out of 132 patches done \n","44 out of 132 patches done \n","45 out of 132 patches done \n","46 out of 132 patches done \n","47 out of 132 patches done \n","48 out of 132 patches done \n","49 out of 132 patches done \n","50 out of 132 patches done \n","51 out of 132 patches done \n","52 out of 132 patches done \n","53 out of 132 patches done \n","54 out of 132 patches done \n","55 out of 132 patches done \n","56 out of 132 patches done \n","57 out of 132 patches done \n","58 out of 132 patches done \n","59 out of 132 patches done \n","60 out of 132 patches done \n","61 out of 132 patches done \n","62 out of 132 patches done \n","63 out of 132 patches done \n","64 out of 132 patches done \n","65 out of 132 patches done \n","66 out of 132 patches done \n","67 out of 132 patches done \n","68 out of 132 patches done \n","69 out of 132 patches done \n","70 out of 132 patches done \n","71 out of 132 patches done \n","72 out of 132 patches done \n","73 out of 132 patches done \n","74 out of 132 patches done \n","75 out of 132 patches done \n","76 out of 132 patches done \n","77 out of 132 patches done \n","78 out of 132 patches done \n","79 out of 132 patches done \n","80 out of 132 patches done \n","81 out of 132 patches done \n","82 out of 132 patches done \n","83 out of 132 patches done \n","84 out of 132 patches done \n","85 out of 132 patches done \n","86 out of 132 patches done \n","87 out of 132 patches done \n","88 out of 132 patches done \n","89 out of 132 patches done \n","90 out of 132 patches done \n","91 out of 132 patches done \n","92 out of 132 patches done \n","93 out of 132 patches done \n","94 out of 132 patches done \n","95 out of 132 patches done \n","96 out of 132 patches done \n","97 out of 132 patches done \n","98 out of 132 patches done \n","99 out of 132 patches done \n","100 out of 132 patches done \n","101 out of 132 patches done \n","102 out of 132 patches done \n","103 out of 132 patches done \n","104 out of 132 patches done \n","105 out of 132 patches done \n","106 out of 132 patches done \n","107 out of 132 patches done \n","108 out of 132 patches done \n","109 out of 132 patches done \n","110 out of 132 patches done \n","111 out of 132 patches done \n","112 out of 132 patches done \n","113 out of 132 patches done \n","114 out of 132 patches done \n","115 out of 132 patches done \n","116 out of 132 patches done \n","117 out of 132 patches done \n","118 out of 132 patches done \n","119 out of 132 patches done \n","120 out of 132 patches done \n","121 out of 132 patches done \n","122 out of 132 patches done \n","123 out of 132 patches done \n","124 out of 132 patches done \n","125 out of 132 patches done \n","126 out of 132 patches done \n","127 out of 132 patches done \n","128 out of 132 patches done \n","129 out of 132 patches done \n","130 out of 132 patches done \n","131 out of 132 patches done \n"]}]},{"cell_type":"code","source":["imgOutputDir=os.path.join(OUTPUT_DIR,imgName[:-4])\n","print(imgOutputDir)\n","if not os.path.exists(imgOutputDir):\n","    os.mkdir(imgOutputDir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86xuLbmU5Kqp","executionInfo":{"status":"ok","timestamp":1641749483053,"user_tz":-300,"elapsed":4,"user":{"displayName":"usama shahid","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03571715957292202472"}},"outputId":"418e4042-a227-4d28-e916-64b21a9cd2ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/FiverrOrders/22_01_05Munazza/Outputs/060102w\n"]}]},{"cell_type":"code","source":["\n","display_instances(img, np.array([[0,0,img.shape[0],img.shape[1]]]),OriginalMask, np.array([1]), \n","                            ['BG','Building'], np.array([1]), \n","                            title=\"Original Mask \",show_mask=False,show_bbox=False)\n","SaveMaskToFile(os.path.join(imgOutputDir,\"OriginalMask.tif\"),OriginalMask)\n","plt.savefig(os.path.join(imgOutputDir,\"OriginalMaskPlt.tif\"),bbox_inches='tight')\n","plt.show()\n","\n","display_instances(img, np.array([[0,0,img.shape[0],img.shape[1]]]),PredictedMask, np.array([1]), \n","                            ['BG','Building'], np.array([1]), \n","                            title=\"Predicted Mask \",show_mask=False,show_bbox=False)\n","\n","SaveMaskToFile(os.path.join(imgOutputDir,\"PredictedMask.tif\"),PredictedMask)\n","plt.savefig(os.path.join(imgOutputDir,\"PredictedMaskPlt.tif\"),bbox_inches='tight')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1WldBD53DNGZJuFaZoVAUgX8wJlwJmbuk"},"id":"6roZviVVkTpg","executionInfo":{"status":"ok","timestamp":1641750812097,"user_tz":-300,"elapsed":103913,"user":{"displayName":"usama shahid","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03571715957292202472"}},"outputId":"03f729be-b2aa-43e0-a6e5-c4106d6a0e2a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Compare Both masks and find which new buildings have been dicovered by the model that were not present in original mask."],"metadata":{"id":"aEakAnQ18eKp"}},{"cell_type":"code","source":["newMask=((PredictedMask*255)-(OriginalMask*255))>0\n","newMask=RemoveLowSizedInstances(newMask, 300)\n"],"metadata":{"id":"0r4tTEkIkfAX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_instances(img, np.array([[0,0,img.shape[0],img.shape[1]]]),newMask, np.array([1]), \n","                            ['BG','Building'], np.array([1]), \n","                            title=\"Subtracted Mask \",show_mask=False,show_bbox=False)\n","\n","SaveMaskToFile(os.path.join(imgOutputDir,\"SubtractedMask.tif\"),newMask)\n","plt.savefig(os.path.join(imgOutputDir,\"SubtractedMaskPlt.tif\"),bbox_inches='tight')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55,"output_embedded_package_id":"1QXofs02SinLkn0QwuA5U7vj2Ri-6rhYR"},"id":"OJSYginXxWx1","executionInfo":{"status":"ok","timestamp":1641750816610,"user_tz":-300,"elapsed":4518,"user":{"displayName":"usama shahid","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03571715957292202472"}},"outputId":"1b282402-138a-490a-efe9-c92bf959c3a4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[""],"metadata":{"id":"Z3RynfZtxZOj"},"execution_count":null,"outputs":[]}]}